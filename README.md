# Machine Learning Project 2

## About the project

This is a project in CS-433 Machine Learning course at EPFL on the AIcrowd. In this project, we implement binary classifiers of text sentiment (negative and positive emotions), with the help of machine learning and deep learning algorithms.

The algorithms we implemented are as follows:

- Support Vector Machine (SVM)

- Random Forest

- Fasttext

- BERT

- RoBERTa

For more details on the background of the project, please check out AIcrowd's [project homepage](https://www.aicrowd.com/challenges/epfl-ml-text-classification).

## File Structure

- In ./data, you can find data files. (Please follow the instructions on the project homepage to download the relevant data files and place them in this folder.)

- In ./, you can find a script main_SVM_RF_Fasttext, which is to run first three implemented algorithms (SVM, Random Forest and Fasttext).

- In ./, you can find a environmental.yml file, which contains all dependencies or libraries we need in this project.

- In ./src_SVM_RF_Fasttext, you can more python files about implementation of the three algorithms, which contains:
  
  - data_loading.py, which is used to store the functionsthat implement data reading.
  
  - data_save.py, which is used to store the functions about data storing.
  
  - pre_processing.py, which contains a class Pre_processing to implement all data pre-processing relevant methods.
  
  - vectorization.py, which contains a class Vectorization to implement all kinds of vectorization/word embedding methods.
  
- In ./src_bert, you can find source code for fine-tuning BERT-based models. 
  For some of the python scripts, we also provide corresponding ipynb version which can be run directly on Google Colab. 
  Please track for the parameters at beginning of the file before running any script.
  You can find inside this directory:
 
  -  train.py, which performs complete fine-tuning of the pre-trained BERT model and store the final model to the directory ./models (please make sure this directory always exists before running this script).
  
  - run.py, which load a torch model produced by the file train.py and create in the directory ./data/ a file sample_submission.csv .
  
  - preprocess.py, which performs basic cleaning on the data and split original data into two files, one for training and the other for validation. 
  
  - build_model.py, in which our models and training process are defined. 
  
  - dataloader.py, in which a torch Dataset class is defined.

## About the data

The data is provided by ML&O EPFL on [AIcrowd](https://www.aicrowd.com/challenges/epfl-ml-text-classification/dataset_files). There is a .zip file which contains following six files:

- train_pos.txt

- train_neg.txt

- train_pos_full.txt

- train_neg_full.txt

- text_data.txt

- sampleSubmission.csv

where the files with no full in its names are small batches of the one with full files, and text_data.txt is for submission.

## Getting Started

### Prerequisites

You can find a list of required libraries that you need in environment.yml file.

OR

You can use this file directly to install them by

    conda env create --file environment.yml --name env-name

It is worth noting that you will need the transformer and its corresponding pre-trained model. You can install the transformer with the following command:

    pip install -U sentence-transformers

## Usage

You can run the script main_SVM_RF_Fasttext.py to see the implementation of the three algorithms as well as the training results.

Here is an example of how to use main_SVM_RF_Fasttext.py with training algorithm SVM, a small batch dataset of 10000 and word embedding method Count Vectorizer:

    python main_SVM_RF_Fasttext.py -m SVM -bn 100000 -mwe CV

Here are the detail of arguments about this .py file:

- -m, --method: name of training method ("SVM", "RF", "Fasttext")

- -bn, --batch_number: number of batch size

- -full, --full: Choose if use the full dataset (True, False)

- -mwe, --method_word_embedding: name of word embedding method ("CV", "TF", "NGRAM", "PRE")



## Best Model

The file <sample_submission.csv> in ./data/ is our best model, generated by method "BERT".

The accuracy can read 0.859 on AIcrowd.



## Authors

- Yifei Song (yifei.song@epfl.ch)

- Haoming Lin (haoming.lin@epfl.ch)

- Ruiqi Yu (ruiqi.yu@epfl.ch)
